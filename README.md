# Awesome AI leaderboards

Leaderboards, benchmarks, and datasets to help you choose your model üßê

| Name | Paper | Code | Leaderboard | Evaluation |
|------|-------|-----------|-------------|------------|
| Chatbot Arena | [Paper](https://arxiv.org/abs/2403.04132) | [Code](https://github.com/lm-sys/FastChat) | [Leaderboard](https://arena.lmsys.org/) | Alignment with human preferences |
| ü§ó Open LLM Leaderboard | | See About: Reproducability | [Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) | Various (generative models) |
| HellaSwag | [Paper](https://rowanzellers.com/hellaswag/) | [Code](https://rowanzellers.com/hellaswag/) | [Leaderboard](https://rowanzellers.com/hellaswag/) | Commonsense reasoning |
| Optimium Benchmark | | [Code](https://github.com/huggingface/optimum-benchmark) | [Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard) | Computational performance |
| BEIR | [Paper](https://openreview.net/forum?id=wCu6T5xFjeJ) | [Code](https://github.com/beir-cellar/beir) | [Leaderboard](https://eval.ai/web/challenges/challenge-page/1897/overview) | Information retrieval |
| Massive Text Embedding (MTEB) | [Paper](https://arxiv.org/abs/2210.07316) | [Code](https://github.com/embeddings-benchmark/mteb?tab=readme-ov-file) | [Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) | Embedding |
| Berkeley Function-Calling Leaderboard | [Paper](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html) | [Code](https://github.com/ShishirPatil/gorilla) | [Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html) | Function calling |
| DecodingTrust | [Paper](https://arxiv.org/abs//2306.11698) | [Code](https://github.com/AI-secure/DecodingTrust) | [Leaderboard](https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard) | Safety |
| SuperGLUE | [Paper](https://arxiv.org/abs/1905.00537) | [Code](https://jiant.info/) | [Leaderboard](https://super.gluebenchmark.com/leaderboard) | Language understanding |

Also, here's some tools worth mentioning:

* [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness): framework for evaluating against over 60 academic benchmarks (used by Hugging Face's Open LLM Leaderboard.
